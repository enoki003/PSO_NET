# PSO駆動型動的結合ネットワーク研究の詳細設計

## 0. 実装サマリと利用手順

### 0.1 実装状況

- CIFAR-100 向けサブ専門家トレーナー（`program/train_sub.py`）を実装済み。層化サブセット抽出・データ拡張・softmax/logits 切替に対応。
- PSO ゲーティング最適化パイプライン（`program/pso_train.py`／`program/train_pipeline.py`）を整備し、平均結合行列を履歴 (`avg_gating`) として記録。
- ゲーティング遷移のネットワークアニメーション（`program/visualize_gating.py`）で GIF / MP4 出力とインタラクティブ確認が可能。
- 評価指標は精度・冗長性・複雑度・滑らかさを JSON で出力。FLOPs・モジュラリティの自動算出と継続学習シナリオは未実装。

注：現行の最小実装として MNIST サブ NN 学習（`program/train_sub_nn.py`）が含まれます。まずはこれで環境確認が可能。

### 0.2 セットアップと基本コマンド（uv 前提）

```bash
# 依存関係の同期（.venv が作成されます）
uv sync

# MNIST サブ NN の学習（ルートでモジュール実行）
uv run -m program.train_sub_nn

# もしくは、インストール済みエントリポイント（uv sync 後）
uv run train-sub-nn

# 出力: program/models/test_model/ 配下（デフォルト）
```

注意:
- Python バージョンは TensorFlow の互換性上、`>=3.10,<3.13` を使用します。本リポジトリでは `.python-version` を `3.12` に固定しています。uv のメッセージに従って `uv python pin 3.12` を実行しても同等です。
- コマンドは必ずリポジトリのルートで実行してください（`program/` 直下からファイル実行すると相対インポートが失敗します）。

## 1. 背景と動機

### 1.1 静的な深層学習の課題

現代の深層学習は固定的なネットワーク構造（CNN や Transformer）の上で勾配法によりパラメータを学習する手法が主流である。しかし固定構造では全入力に対して同じ計算グラフを実行するため計算効率に限界があり、未知状況への適応性や継続学習時の「破滅的忘却」に弱い。動的ニューラルネットワークはこの硬直性を克服するため、入力ごとに計算グラフや重みを適応的に変化させるモデルであり、研究では複数の動的幅モデルが報告されている（例：[pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov)）。特に動的幅を実現するためにはゲーティング機構が重要であり、入力から各モジュールの活性度を調整して実行幅を選択する仕組みが計算効率と適応性・表現力・解釈可能性の向上に寄与する。

### 1.2 Mixture-of-Experts (MoE) とゲーティングモデル

Mixture-of-Experts はサブタスクごとに訓練された複数の専門家モデル（expert）と、それらの貢献度を入力に応じて決めるゲーティングネットワーク（gating model）から構成される。ゲーティングネットワークは入力パターンに基づき各専門家への重み（ソフトマックス確率）を出力し、どの専門家をどの程度信頼するかを制御する（参考：[machinelearningmastery.com](https://machinelearningmastery.com)）。動的な重み付けにより、入力領域の特性に応じた柔軟な分割統治が実現できる。

### 1.3 Particle Swarm Optimization (PSO)

PSO は鳥や魚の群れの行動に着想を得た進化的計算アルゴリズムで、粒子（解候補）群が反復的に位置と速度を更新しながら適応度関数を最大化（あるいは最小化）する。各粒子の速度更新は自己の過去最良位置（個体知識）と群全体の最良位置（社会知識）に引き寄せられる形で行われ、位置はこの速度で移動する。PSO は勾配情報を必要としないため、非微分的な罰則を含む目的関数にも適用しやすく、初期値への依存が小さい。[pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov) などで詳細が整理されており、全結合 (gbest) と局所結合 (lbest) のトポロジ選択によって探索の多様性も調整できる。

### 1.1 静的な深層学習の課題

現代の深層学習は固定的なネットワーク構造（CNN や Transformer）の上で勾配法によりパラメータを学習する手法が主流である。しかし固定構造では全入力に対して同じ計算グラフを実行するため計算効率に限界があり、未知状況への適応性や継続学習時の「破滅的忘却」に弱い。動的ニューラルネットワークはこの硬直性を克服するため、入力ごとに計算グラフや重みを適応的に変化させるモデルであり、研究では複数の動的幅モデルが報告されている（例：[pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov)）。特に動的幅を実現するためにはゲーティング機構が重要であり、入力から各モジュールの活性度を調整して実行幅を選択する仕組みが計算効率と適応性・表現力・解釈可能性の向上に寄与する。

### 1.2 Mixture-of-Experts (MoE) とゲーティングモデル

Mixture-of-Experts はサブタスクごとに訓練された複数の専門家モデル（expert）と、それらの貢献度を入力に応じて決めるゲーティングネットワーク（gating model）から構成される。ゲーティングネットワークは入力パターンに基づき各専門家への重み（ソフトマックス確率）を出力し、どの専門家をどの程度信頼するかを制御する（参考：[machinelearningmastery.com](https://machinelearningmastery.com)）。動的な重み付けにより、入力領域の特性に応じた柔軟な分割統治が実現できる。

### 1.3 Particle Swarm Optimization (PSO)

PSO は鳥や魚の群れの行動に着想を得た進化的計算アルゴリズムで、粒子（解候補）群が反復的に位置と速度を更新しながら適応度関数を最大化（あるいは最小化）する。各粒子の速度更新は自己の過去最良位置（個体知識）と群全体の最良位置（社会知識）に引き寄せられる形で行われ、位置はこの速度で移動する。PSO は勾配情報を必要としないため、非微分的な罰則を含む目的関数にも適用しやすく、初期値への依存が小さい。[pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov) などで詳細が整理されており、全結合 (gbest) と局所結合 (lbest) のトポロジ選択によって探索の多様性も調整できる。

## 2. 研究目的

本研究では、動的な計算グラフを自律的に形成できるニューラルアーキテクチャを構築し、勾配に頼らない PSO によってゲーティング構造を最適化することを目標とする。達成すべきポイントは以下のとおり。

- **動的結合機構の構築**：複数の専門家ネットワークと、入力に基づいて専門家間の結合行列を生成するゲーティングネットワークを設計する。ゲーティング重みは PSO で探索する。
- **計算効率と適応性の向上**：従来の固定アーキテクチャや MoE と比較し、入力ごとに結合を変えることで分類性能や継続学習性能の改善を狙う。
- **創発的構造の分析**：PSO で獲得された結合構造を可視化し、協調・抑制パターンの観察と解釈を行う。

## 3. 理論的基盤と設計

### 3.1 動的ニューラルネットワークとゲーティング

動的ネットワークは入力や時間に応じて構造や重みを変える。動的幅を実現するゲーティング機構では、ゲート関数が入力から各専門家の活性レベルを出力し、必要な専門家のみを選択する（例：[pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov)）。MoE のゲーティングモデルは入力に応じて専門家の貢献度を決定し、その出力（ソフトマックス確率）が専門家の重みとして用いられる（[machinelearningmastery.com](https://machinelearningmastery.com)）。

本研究ではゲーティングネットワークを結合行列生成関数として一般化する。入力 \(x\) に対してゲーティングネットワーク \(g_\theta\) が専門家数 \(N\) の結合行列 \(C_x \in \mathbb{R}^{N \times N}\) を生成し、専門家同士の情報交換強度を決定する。従来の MoE は一次元の重みベクトルを出力して専門家の線形和を取るが、本研究では時間方向の再帰的情報交換を導入し、専門家間の協調・競合関係を学習する。結合行列は非対称を許容し、長期的な情報流を表現できるようにする。

### 3.2 PSO によるゲーティング関数の最適化

粒子 \(i\) の速度・位置更新は次式で与えられる：

\[
\mathbf{v}_i(t+1) = \omega \mathbf{v}_i(t) + c_1 r_1 (p_i - \mathbf{x}_i(t)) + c_2 r_2 (g - \mathbf{x}_i(t)), \quad
\mathbf{x}_i(t+1) = \mathbf{x}_i(t) + \mathbf{v}_i(t+1)
\]

ここで \(\omega\) は慣性重み、\(c_1, c_2\) は認知係数・社会係数、\(r_1, r_2\) は一様乱数ベクトルである。位置更新は勾配を参照せずデータ駆動的に進むため、非微分な目的関数にも適用できる。

適応度関数の例：

\[
F(\theta) = \alpha \cdot \text{Acc} - \beta \cdot \text{Redundancy} - \gamma \cdot \text{Complexity} - \delta \cdot \text{Smoothness}
\]

- **Acc**：分類精度（トップ1／トップ5）。
- **Redundancy**：専門家出力の相関（多様性確保）。
- **Complexity**：結合行列の密度や計算量に対する罰則。
- **Smoothness**：再帰ステップ間の結合行列変化量に対する罰則。

慣性・係数 (\(\omega, c_1, c_2\)) やトポロジ設定 (gbest / lbest) を調整することで探索と収束のバランスを取る。

### 3.3 モデルアーキテクチャ

#### 3.3.1 専門家ネットワーク (Sub-NN)

- CIFAR-100 を扱う小規模 CNN を採用。Conv-BN-ReLU ブロック＋Global Average Pooling＋全結合層で 100 クラスのロジットを出力。
- Dropout やラベルスムージングで過学習を抑制し、各専門家は異なるサブセット（10〜20%）で事前学習する。
- Softmax 版モデルを事前学習に用い、PSO ではロジット版を読み込む。

#### 3.3.2 ゲーティングネットワーク

- 入力：画像または専門家の中間特徴（32×32×3 など）。
- 出力：\(N \times N\) の結合行列 \(C_x\)。行ごとにソフトマックス正規化し、遷移確率として解釈する。自己結合を許容するかは実験で検証する。
- 構造：軽量 CNN または MLP で特徴を圧縮し、最終層で \(N \times N\) の線形出力を得る。

#### 3.3.3 再帰的情報交換

- 初期の専門家出力ベクトル \(O^{(0)} \in \mathbb{R}^{N \times C}\) に対し、結合行列による線形変換と正規化を \(T\) ステップ反復（\(O^{(t+1)} = \phi(C_x O^{(t)})\)）。
- \(\phi\) は恒等写像＋LayerNorm 等の安定化処理を想定。最終出力は各専門家ベクトルを平均または重み付き和で集約する。

### 3.4 タスクと実験設計

**基礎性能検証 (CIFAR-100)**

- データセット：CIFAR-100（100 クラス、32×32×3）。
- 専門家数：8〜10。
- 事前学習：各専門家を 10〜20% の層化サブセットで学習し、トップ1精度 35〜55% 程度に抑える。
- PSO 設定：粒子数 30〜50、反復回数 300〜500、慣性重みを 0.9 → 0.4 へ線形減衰、認知・社会係数を 1.5〜2.0。

### 4.3 可視化と解析

- PSO 反復ごとに平均結合行列をヒートマップ／ネットワークグラフで可視化し、結合パターンの変化を観察。
- 専門家の出力を UMAP や t-SNE で低次元化し、クラス分布と多様性を分析。
- 適応度や粒子多様性（速度分散・位置分散）の推移を追跡し、探索と収束の挙動を評価。

## 5. 理論的検討と予想

- **動的構造適応と計算効率**：動的幅モデルは入力ごとに計算リソースを調整でき、不要な専門家を抑制しながら精度を維持できる。動的深さ／幅／ルーティングは結合行列生成で統一的に表現可能。
- **PSO の利点**：非微分な罰則項を含む目的関数でも最適化でき、初期値や学習率への感度が小さい。局所トポロジや慣性調整で多峰性空間でも探索が安定する。
- **専門家多様性の重要性**：冗長性ペナルティやサブセット分割、ラベルスムージングにより専門家間の多様性を確保し、意味のある結合学習を促進する。
- **期待される成果**：固定アーキテクチャや標準 MoE よりも高い精度を目指し、継続学習・クラス不均衡への耐性向上を期待。結合行列の可視化で入力依存構造の解釈性を得る。

## 6. 実施計画

1. **準備**：CIFAR-100 前処理、専門家ネットワーク／ゲーティングネットワーク実装、PSO ベクトル化ユーティリティ、実験ログ基盤を整備。
2. **事前学習実験**：専門家ごとの精度と冗長性を測定し、サブセット割合やモデルサイズを調整。
3. **PSO 実験**：粒子数・慣性・係数・トポロジのグリッド探索を行い、目的関数重み (\(\alpha, \beta, \gamma, \delta\)) の感度分析を実施。
4. **比較実験**：固定 CNN、平均アンサンブル、勾配による MoE、動的深さモデルと性能・計算量・メモリ使用量を比較。
5. **継続学習実験**：タスク追加時の性能変化と結合行列変動を分析し、破滅的忘却の軽減効果を評価。
6. **論文化・可視化**：結合ヒートマップ、PSO ダイナミクス、専門家関係図、性能比較グラフなどを整理し、成果をまとめる。

### 6.1 実験手順

以下は CIFAR-100 を用いた標準的な一連の実験フローです。各ステップは再現性確保のためシード指定を推奨します。

1. 環境準備
	- Python 仮想環境作成・有効化。
	- 依存をインストール: `pip install -e .`。
	- GPU 利用確認 (`tensorflow` が GPU を認識するか `tf.config.list_physical_devices('GPU')`)。

2. 専門家事前学習 (Sub-Experts)
	- CIFAR-100 をロードし、`subset_pool_fraction` でクラスごとにプールするサンプルを決定（例: 0.2）。
	- `program/train_sub.py` を利用し層化分割されたインデックス集合で各専門家を学習。
	- Softmax 版を学習し、その後ログイット版へ重みをコピー (`logits.weights.h5`)。

	```bash
		 # 8 専門家を層化 20% プールで 3 epoch 学習
			 # uv のモジュール実行（推奨）
			 uv run -m program.train_sub \
	  --num-experts 8 \
	  --subset-pool-fraction 0.2 \
	  --epochs 3 \
	  --batch-size 128 \
	  --label-smoothing 0.1 \
	  --learning-rate 1e-3 \
	  --subset-seed 42 \
	  --output-root ./models/cifar_sub_experts

			 # エントリポイント（uv sync 後）
			 uv run train-sub \
				 --num-experts 8 \
				 --subset-pool-fraction 0.2 \
				 --epochs 3 \
				 --batch-size 128 \
				 --label-smoothing 0.1 \
				 --learning-rate 1e-3 \
				 --subset-seed 42 \
				 --output-root ./models/cifar_sub_experts
	```

	生成物: `models/cifar_sub_experts/expert_XX/` 下に `history.json`, `softmax.weights.h5`, `logits.weights.h5`, `train_indices.npy`。

3. PSO 用評価セット構築
	- 全専門家の `logits.weights.h5` をロード。
	- CIFAR-100 全体（またはランダムサブセット `--sample-count`）に対する各専門家のロジットを前計算しメモリ保持。
	- これが `program/pso_train.py` 内 `precompute_expert_logits(...)` により `expert_logits` テンソルとして構築される。

4. PSO によるゲーティング最適化
	- ゲーティングモデル（入力 → N×N 行列）を構築し `WeightAdapter` でフラット化。
	- 粒子初期化：モデル重みに微小ノイズを加えたベクトル。
	- 各反復で FitnessEvaluator が適応度 (Acc, Redundancy, Complexity, Smoothness) を計算。
	- 慣性重みは線形減衰（`config.PSO_INERTIA_MAX`→`MIN`）。

		 ```bash
		 # uv のモジュール実行（推奨）
		 uv run -m program.pso_train \
	  --experts ./models/cifar_sub_experts \
	  --num-experts 8 \
	  --sample-count 4096 \
	  --batch-size 128 \
	  --hidden-units 384 \
	  --lr 1e-3 \
	  --seed 123 \
	  --iterations 120 \
	  --particles 24 \
	  --output ./models/pso_gating

		 # エントリポイント（uv sync 後）
		 uv run pso-train \
			 --experts ./models/cifar_sub_experts \
			 --num-experts 8 \
			 --sample-count 4096 \
			 --batch-size 128 \
			 --hidden-units 384 \
			 --lr 1e-3 \
			 --seed 123 \
			 --iterations 120 \
			 --particles 24 \
			 --output ./models/pso_gating
	```

	出力: `pso_history.json` (各 iteration のスナップショット), `gating_weights.npy` (最良ベクトル), `fitness.json`。

5. 再帰的結合評価
	- `config.PSO_RECURRENT_STEPS` 回だけ専門家ロジット集合をゲーティング行列で更新し平均化。
	- 再帰ステップごとの差分ノルムが Smoothness に寄与。

6. 可視化
	- 平均ゲーティング行列の時系列をネットワークアニメーション化：
		 ```bash
		 uv run -m program.visualize_gating \
	  --history ./models/pso_gating/pso_history.json \
	  --out ./models/pso_gating/gating_anim.gif \
	  --threshold 0.02 \
	  --fps 6 --show

		 # エントリポイント
		 uv run viz-gating \
			 --history ./models/pso_gating/pso_history.json \
			 --out ./models/pso_gating/gating_anim.gif \
			 --threshold 0.02 \
			 --fps 6 --show
	```
	- `avg_gating` のヒートマップ生成は任意のノートブックで `plt.imshow(np.array(entry['avg_gating']))`。

7. アブレーション実験
	- 冗長性ペナルティ除去: `config.FITNESS_BETA = 0.0`。
	- 再帰無効化: `config.PSO_RECURRENT_STEPS = 1`。
	- トポロジ差分: 現状 gbest のみ → lbest を導入するには `ParticleSwarmOptimizer` を近傍集合更新へ拡張。
	- 勾配ベース比較: ゲーティングモデルを通常の `model.fit` で学習（専用スクリプト要追加）。

8. 継続学習シナリオ（将来拡張）
	- タスク分割: クラス集合を時間順に分割 (例: 20 クラスずつ)。
	- 各タスクで専門家を追加／既存専門家固定。
	- 追加クラスに対して再度 PSO 最適化し旧タスク精度変化を測定。
	- 指標: 旧タスク保持率 = 新タスク後の旧タスク精度 / 旧タスク学習直後の精度。

9. 結果整理
	- 主要指標とハイパーパラメータを `results.csv` などに集約。
	- 冗長性 vs 精度 のトレードオフ曲線、慣性減衰カーブ、ゲート行列モジュラリティ（未実装）を可視化。

### 6.2 実験成果物一覧

| 種類 | パス例 | 内容 |
|------|--------|------|
| 専門家重み | `models/cifar_sub_experts/expert_00/logits.weights.h5` | ロジット版学習済み CNN |
| 学習履歴 | `models/cifar_sub_experts/expert_00/history.json` | epoch ごとの loss / acc |
| PSO 履歴 | `models/pso_gating/pso_history.json` | iteration, inertia, best_score, avg_gating |
| 最終ゲート | `models/pso_gating/gating_weights.npy` | 最良ゲーティング重みベクトル |
| 適応度 | `models/pso_gating/fitness.json` | 最終 score 各指標値 |
| 可視化 | `models/pso_gating/gating_anim.gif` | ゲート行列時系列アニメーション |

### 6.3 再現性メモ

- 乱数シード: 専門家分割 `--subset-seed`, PSO 最適化 `--seed` を明示記録。
- バージョン: TensorFlow/Keras のバージョンと GPU ドライバ情報を `ENVIRONMENT.md`（未作成）へ追加推奨。
- 計測: 追加で FLOPs や推論レイテンシを取得する場合は `tf.profiler` / `thop` 相当の補助スクリプトを別途作成予定。

## 7. まとめ

本研究は、専門家間の結合行列を PSO によって自己組織的に最適化する動的ニューラルネットワークを提案する。動的幅と再帰的結合により、入力やタスクに応じてネットワーク構造を適応的に変化させることが可能となる。PSO は非微分な罰則を含む複雑な目的関数にも対応でき、勾配法では得にくい大域探索能力を活かせる。計算効率・適応性・容量・解釈性の利点を備え、継続学習や資源制約環境への応用が期待される。
結合構造の可視化：PSO反復ごとにクラス別平均結合行列をヒートマップとして描画し、結合パターンがタスクや入力に応じてどのように変化するかを観察する。また、再帰ステップごとの結合強度の収束過程をアニメーションとして表示する。

専門家の役割分析：学習後に各専門家の出力をUMAPやt‑SNEで低次元に可視化し、各クラスに対する専門家の分布を調べる。冗長性ペナルティの効果を、相関分布のヒストグラムなどで示す。

PSOダイナミクス解析：適応度関数の推移と粒子多様性（速度分散や位置分散）を追跡し、探索段階と収束段階の挙動を評価する。

5 理論的検討と予想

動的構造適応と計算効率：動的幅モデルは入力ごとに計算リソースを変えるため、不要な専門家を抑制して計算量を減らしながら精度を維持できる。人間の脳が局所的に活性するニューロンのみを使用するのに似ている。動的深さ／幅／ルーティングは本研究の結合行列生成に統一的に表現できる。

PSOの利点：PSOは勾配を計算しないため、損失に非微分要素（冗長性やスムーズネス罰則）が含まれていてもそのまま最適化できる。また、初期重みや学習率の調整に過度に敏感でない。局所トポロジの設定や慣性重みの調整により、多峰性の構造空間でもモード崩壊を避けつつ探索が可能となる。

専門家多様性の重要性：冗長性を抑え専門家が異なる特徴を検出できるようにすることが、ゲートが意味のある結合パターンを学習するための前提となる。したがって、事前学習ではデータサブセットの非重複配分やラベルスムージング、ノイズ付加によって専門家の出力分布を多様化する。

期待される成果：本手法が従来の固定アーキテクチャや標準MoEよりも高い精度を達成し、特に継続学習やクラス不均衡における性能保持に優れることを期待する。また、PSOにより得られた結合行列が入力やタスクに応じて意味のある構造を持つことが、可視化を通じて確認されると予想する。

6 実施計画

準備段階：CIFAR‑100のデータ前処理、専門家ネットワークとゲーティングネットワークの実装（Keras/PyTorch）。PSOアルゴリズムのベクトル化関数を実装。実験を管理するスクリプトとログ保存機構を構築。

事前学習実験：複数の専門家の学習を実行し、各専門家の精度と冗長性を確認。適切なサブセット割合やモデルサイズを調整して精度帯を制御。

PSO実験：PSOのパラメータ（粒子数、慣性、認知・社会係数、トポロジ）のグリッド探索を行い、最適化挙動を解析。目的関数の重み lpha,eta,\gamma,\delta の感度分析を実施。

比較実験：従来手法（固定CNN、平均アンサンブル、勾配によるMoE、動的深さモデル）と性能・計算量・メモリ使用量を比較。

継続学習実験：タスクの追加に伴う性能変化を測定し、結合行列の変化を分析。破滅的忘却の軽減度合いを評価。

論文化・可視化：分析結果をまとめ、可視化図表（結合ヒートマップ、PSOダイナミクス、専門家間関係図、性能比較グラフ）を作成。学術論文として投稿を目指す。

7 まとめ

本研究は、動的ニューラルネットワークの新たな実装として、専門家間の結合行列をPSOによって自己組織的に最適化するアーキテクチャを提案する。動的幅・リカレント結合という斬新な設計により、入力やタスクに応じてネットワークの構造を適応的に変化させることが可能となる。PSOの利点を利用することで、非微分性の罰則項を含む複雑な目的関数を直接最適化し、勾配法にはない大域探索能力を発揮できる。動的ネットワークが持つ計算効率・適応性・容量・解釈性の利点を生かし、継続学習や資源制約環境での応用を目指す。